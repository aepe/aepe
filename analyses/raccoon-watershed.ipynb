{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "gis",
   "display_name": "gis"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "#sys.path.append(\"C:\\\\Users\\\\mnowatz\\\\Documents\\\\Dev\\\\aepe\")\n",
    "#print(sys.path)\n",
    "import psycopg2\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import database as db\n",
    "from xml.etree.ElementTree import ElementTree, Element, SubElement\n",
    "import io\n",
    "import json\n",
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import apsim.apsim.wrapper as apsim\n",
    "from apsim.apsim.daymet import Weather\n",
    "from apsim.apsim_input_writer import get_date, add_management_year\n",
    "from apsim import run_apsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconn = db.connect_to_db('database.ini')\n",
    "raccoon_2018 = 'raccoon.raccoon_clu_ssurgo_2018'\n",
    "#raccoon = pd.read_sql(f\"SELECT * FROM {raccoon_2018} LIMIT 200;\", dbconn)\n",
    "greene = pd.read_sql(f\"SELECT * FROM {raccoon_2018} WHERE County='Greene';\", dbconn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin_up_corn = json.loads( open( 'crop_jsons/maize.json', 'r' ).read() )\n",
    "spin_up_soybean = json.loads( open( 'crop_jsons/soybean.json', 'r' ).read() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get the counties we are interested in."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a list of all unique entries in a column.\n",
    "\n",
    "Args:\n",
    "    dbconn {database connection} -- connection to postgresql database\n",
    "    table {str} -- table name\n",
    "    id_column (str) -- column of interest.\n",
    "Returns:\n",
    "    list of all unique entries in a table column\n",
    "'''\n",
    "def get_distinct(dbconn, table, id_column):\n",
    "    entries = pd.read_sql(f'SELECT DISTINCT {id_column} FROM {table};', dbconn)\n",
    "    entries = entries[id_column].tolist()\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bv_query = 'SELECT * FROM raccoon.raccoon_clu_ssurgo_2018 WHERE fips = \\'IA021\\';'\n",
    "'''Get info for a county of interest from a geopandas df\n",
    "Args:\n",
    "    dbconn {database connection} -- connection to postgresql database\n",
    "    table {str} -- name of geopd table\n",
    "    fips {str} -- fips id of the desired county eg. 'IA021'\n",
    "    geom {str} -- column name that contains shape geometry\n",
    "Returns:\n",
    "    geopandas dataframe with county info\n",
    "'''\n",
    "def get_county(dbconn, table, fips, geom, limit=False, limit_num=100):\n",
    "    #Get watershed as geopandas df\n",
    "    if limit:\n",
    "        query = f'SELECT * FROM {table} WHERE fips = \\'{fips}\\' LIMIT {limit_num};'\n",
    "    else:\n",
    "        query = f'SELECT * FROM {table} WHERE fips = \\'{fips}\\';'\n",
    "    county_gpd = gpd.read_postgis(query, dbconn, geom_col=geom)\n",
    "    return county_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buena_vista = get_county(dbconn, 'raccoon.raccoon_clu_ssurgo_2018', 'IA021', \"wkb_geometry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get the county centroid for creating APSIM met files"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find and return the centroid of a geopandas geometry\n",
    "\n",
    "Args: \n",
    "    geodf {dataframe} -- geopandas dataframe\n",
    "    id {string} -- the id of interest in the geodf (e.g., 'fips' for county column)\n",
    "    geometry (string) - geopd column with geometries\n",
    "\n",
    "Returns:\n",
    "    {np.array} -- lat and longitude of geometry\n",
    "'''\n",
    "def get_centroid(geodf, id, geometry):\n",
    "    #get the geometry of interest by id - 'fips' for a county\n",
    "    geom = geodf[[id, geometry]]\n",
    "    #dissolve geometries to make one big geometry\n",
    "    dissolved_geom = geom.dissolve(by=id)\n",
    "    #find the centroid of the dissolved geometry and return its long, lat\n",
    "    centroid = dissolved_geom[geometry].centroid\n",
    "    coords = np.vstack([centroid.x, centroid.y]).T\n",
    "    #change to array and lat, long\n",
    "    centroid_coords = np.array([coords[0][1], coords[0][0]])\n",
    "    return centroid_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bv_centroid = get_centroid(buena_vista, 'fips', \"wkb_geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the weather for given centroid and write to a .met file\n",
    "\n",
    "Args:\n",
    "    lat {float} -- latitude of centroid\n",
    "    long {float} -- longitude of centroid\n",
    "    year_star {int} -- starting year of weather data\n",
    "    year_end {int} -- ending year of weather data\n",
    "    path {str} -- path to write the met files\n",
    "    filename {str} -- name to give the .met file\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "'''\n",
    "def create_met(lat, long, start_year, end_year, filename, path='apsim_files/met_files'):\n",
    "    weather_obj = Weather().from_daymet(lat, long, start_year, end_year)\n",
    "    weather_obj.write_met_file(f'{path}/{filename}.met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "greene = get_county(dbconn, 'raccoon.raccoon_clu_ssurgo_2018', 'IA073', \"wkb_geometry\")\n",
    "greene_centroid = get_centroid(greene, 'fips', \"wkb_geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "greene = Weather().from_daymet(greene_centroid[0], greene_centroid[1], 1980, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ParserError",
     "evalue": "Passed header=6 but only 1 lines in file",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-f4df9cb46d89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_met\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreene_centroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreene_centroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1980\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'greene'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-6eaee119a931>\u001b[0m in \u001b[0;36mcreate_met\u001b[1;34m(lat, long, start_year, end_year, filename, path)\u001b[0m\n\u001b[0;32m     14\u001b[0m '''\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_met\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'apsim_files/met_files'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mweather_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWeather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_daymet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mweather_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_met_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{path}/{filename}.met'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mnowatz\\documents\\dev\\aepe\\apsim\\apsim\\daymet.py\u001b[0m in \u001b[0;36mfrom_daymet\u001b[1;34m(self, lat, lon, startyr, endyr)\u001b[0m\n\u001b[0;32m    102\u001b[0m         }\n\u001b[0;32m    103\u001b[0m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mDAYMET_URL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpayload\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mwth_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# day of year\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Passed header=6 but only 1 lines in file"
     ]
    }
   ],
   "source": [
    "create_met(greene_centroid[0], greene_centroid[1], 1980, 2019, 'greene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "greene_df = greene.data\n",
    "type(greene_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greene_df.to_excel('greene.xlsx', index=False)\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "for r in dataframe_to_rows(green_df, index=False, header=True):\n",
    "    ws.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.insert_rows(1)\n",
    "ws['A1'] = 'This is a test to insert some text.'\n",
    "wb.save(\"test.prn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('test.prn', 'test.met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "greene_weather = create_met(greene_centroid[0], greene_centroid[1], 1980, 2019, 'greene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create all met files for each county in a given geometry.\n",
    "\n",
    "Args:\n",
    "    dbconn {database connection} -- connection to postgresql database\n",
    "    counties {array/list} -- every county to be run on in the geometry\n",
    "    table {str} -- name of the table within the database to get geometries from\n",
    "    id_column {str} -- table column that has unique ids (in this case fips) for each county.\n",
    "    geo_col {str} -- table column that has geometry.\n",
    "    name_col {str} -- column that has the name for each county - can just use fips columns if no county names in table.\n",
    "\n",
    "Returns:\n",
    "    Met file for each county in counties.\n",
    "\"\"\"\n",
    "def create_all_met(dbconn, counties, table, id_col='fips', geo_col='wkb_geometry', name_col='county'):\n",
    "    for i in counties:\n",
    "        county = get_county(dbconn, table, i, geo_col)\n",
    "        county_name = county[name_col][0].replace(\" \", \"_\")\n",
    "        print(f'Geopandas table for {county_name} county created.')\n",
    "        centroid = get_centroid(county, id_col, geo_col)\n",
    "        print(f'Centroid located at {centroid}.')\n",
    "        weather = create_met(centroid[0], centroid[1], 1980, 2019, county_name)\n",
    "        print(f\"Met file for {county_name}/{i} at location {centroid} created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get all distinct counties and create met files for all county geometry lat/lon"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fips = get_distinct(dbconn, raccoon_2018, 'fips')\n",
    "#create_all_met(dbconn, fips, raccoon_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get clukey crop rotations"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the crop rotation for each clukey.\n",
    "\n",
    "Args:\n",
    "    df {obj} -- Dataframe that contains individual clukey information.\n",
    "    crop_column {str} -- Column name that contains the label for what crop is growing for a given year.\n",
    "\n",
    "Returns:\n",
    "    Str of the rotation. e.g., 'cs' = corn-soy\n",
    "\"\"\"\n",
    "def get_rotation(df, crop_column):\n",
    "    #save rotation for clukey to crops list\n",
    "    crops = []\n",
    "    for i in df.index:\n",
    "        val = df.loc[i, crop_column]\n",
    "        if val == 'Corn' or val == 'Soybean':\n",
    "            crops.append(val)\n",
    "        else:\n",
    "            crops.append('other')\n",
    "    #evaluate crops list and return a rotation\n",
    "    if all(x in crops for x in ['Corn', 'Soybean']):\n",
    "        rotation = 'cs'\n",
    "    elif all(x in crops for x in ['Corn']):\n",
    "        rotation = 'cc'\n",
    "    else:\n",
    "        rotation = 'other'\n",
    "    return rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rots = pd.read_sql('SELECT * FROM raccoon.raccoon_rots', dbconn)\n",
    "rots['rotation'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group df by clukey, sort by year, then loop through and get rotation, appending to original df.\n",
    "grouped = rots.groupby('clukey')\n",
    "clukeys = rots.drop_duplicates('clukey')\n",
    "for i in clukeys['clukey']:\n",
    "    field = grouped.get_group(i).sort_values(by=['years'], ascending=True)\n",
    "    rotation = get_rotation(field, 'crop')\n",
    "    rots.loc[rots['clukey'] == i, 'rotation'] = rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Get the soil properties for each individual mukey"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_input_table(dbconn, table, fip, start_year=2016, end_year=2018, id_col='fips', geo_col='wkb_geometry', name_col='county', soil_col=\"mukey\", limit=False):\n",
    "def create_apsim_files(df, rotations_df, dbconn, field_key='clukey', soil_key='mukey', county_col='county', rotation_col='rotation', crop_col='crop', start_year=2016, end_year=2018):\n",
    "    if not os.path.exists('apsim_files'):\n",
    "        os.makedirs('apsim_files')\n",
    "    start_date = f'01/01/{start_year}'\n",
    "    end_date = f'31/12/{end_year}'\n",
    "    #save rotation for clukey to crops list\n",
    "    df = df.drop_duplicates(field_key)\n",
    "    #loop through field keys e.g., clukeys\n",
    "    for i in df[field_key]:\n",
    "        field_id = i\n",
    "        #get field information\n",
    "        #TODO get 'clukey' and 'county' to work as function inputs instead of hardcoded\n",
    "        field = df.loc[df['clukey'] == i]\n",
    "        #get field rotation\n",
    "        rotation_row = rotations_df.loc[rotations_df[field_key] == i]\n",
    "        rotation = get_rotation(rotation_row, crop_col)\n",
    "        #get unique soil keys e.g., mukeys\n",
    "        soils = field.drop_duplicates(soil_key)\n",
    "        #get weather file for desired county\n",
    "        county_name = field.iloc[0]['county'].replace(\" \", \"_\")\n",
    "        met_name = f\"{county_name}.met\"\n",
    "        met_path = f\"met_files/{met_name}\"\n",
    "        #create apsim file for each unique soil in field\n",
    "        for i in soils[soil_key]:\n",
    "            try:\n",
    "                soil_id = i\n",
    "                soil_query = '''select * from api.get_soil_properties( array[{}]::text[] )'''.format( i )\n",
    "                soil_df = pd.read_sql( soil_query, dbconn )\n",
    "                if soil_df.empty:\n",
    "                    continue\n",
    "                #soil_row = soils_df.loc[soils_df[f'{soil_key}'] == i]\n",
    "                #initialize .apsim xml\n",
    "                apsim_xml = Element( 'folder' )\n",
    "                apsim_xml.set( 'version', '36' )\n",
    "                apsim_xml.set( 'creator', 'C-CHANGE Foresite' )\n",
    "                apsim_xml.set( 'name', county_name )\n",
    "                sim = SubElement( apsim_xml, 'simulation' )\n",
    "                sim.set( 'name', f'{county_name} {field_id}' )\n",
    "                \n",
    "                #set met file\n",
    "                metfile = SubElement( sim, 'metfile' )\n",
    "                metfile.set( 'name', f'{county_name}' )\n",
    "                filename = SubElement( metfile, 'filename' )\n",
    "                filename.set( 'name', 'filename' )\n",
    "                filename.set( 'input', 'yes' )\n",
    "                filename.text = met_path\n",
    "\n",
    "                #set clock\n",
    "                clock = SubElement( sim, 'clock' )\n",
    "                clock_start = SubElement( clock, 'start_date' )\n",
    "                clock_start.set( 'type', 'date' )\n",
    "                clock_start.set( 'description', 'Enter the start date of the simulation' )\n",
    "                clock_start.text = start_date\n",
    "                clock_end = SubElement( clock, 'end_date' )\n",
    "                clock_end.set( 'type', 'date' )\n",
    "                clock_end.set( 'description', 'Enter the end date of the simulation' )\n",
    "                clock_end.text = end_date\n",
    "                sumfile = SubElement( sim, 'summaryfile' )\n",
    "                area = SubElement( sim, 'area' )\n",
    "                area.set( 'name', 'paddock' )\n",
    "\n",
    "                # add soil xml\n",
    "                soil = apsim.Soil( soil_df, SWIM = False, SaxtonRawls = False )\n",
    "                area.append( soil.soil_xml() )\n",
    "                ### surface om\n",
    "                surfom_xml = apsim.init_surfaceOM( 'maize', 'maize', 3500, 65, 0.0 )\n",
    "                area.append( surfom_xml )\n",
    "                ### fertilizer\n",
    "                fert_xml = SubElement( area, 'fertiliser' )\n",
    "\n",
    "                ### crops\n",
    "                crop_xml = SubElement( area, 'maize' )\n",
    "                crop_xml = SubElement( area, 'soybean' )\n",
    "                crop_xml = SubElement( area, 'wheat' )\n",
    "\n",
    "                ### output file\n",
    "                outvars = [\n",
    "                    'dd/mm/yyyy as Date', 'day', 'year',\n",
    "                    'yield', 'biomass', 'fertiliser',\n",
    "                    'surfaceom_c', 'subsurface_drain',\n",
    "                    'subsurface_drain_no3', 'leach_no3',\n",
    "                    'corn_buac', 'soy_buac' ]\n",
    "                output_xml = apsim.set_output_variables( f'{county_name}_{field_id}_{soil_id}.out', outvars )\n",
    "                area.append( output_xml )\n",
    "\n",
    "                graph_no3 = [\n",
    "                    'Cumulative subsurface_drain',\n",
    "                    'Cumulative subsurface_drain_no3',\n",
    "                    'Cumulative leach_no3'\n",
    "                ]\n",
    "                graph_yield = [\n",
    "                    'yield',\n",
    "                    'biomass',\n",
    "                    'corn_buac'\n",
    "                ]\n",
    "                graph_all = [\n",
    "                    'yield', 'biomass', 'fertiliser',\n",
    "                    'surfaceom_c', 'Cumulative subsurface_drain',\n",
    "                    'Cumulative subsurface_drain_no3',\n",
    "                    'Cumulative leach_no3', 'corn_buac',\n",
    "                    'soy_buac'\n",
    "                ]\n",
    "\n",
    "                output_xml.append( apsim.add_xy_graph( 'Date', graph_no3, 'no3' ) )\n",
    "                output_xml.append( apsim.add_xy_graph( 'Date', graph_yield, 'yield' ) )\n",
    "                output_xml.append( apsim.add_xy_graph( 'Date', graph_all, 'all outputs' ) )\n",
    "\n",
    "                op_man = apsim.OpManager()\n",
    "                op_man.add_empty_manager()\n",
    "                if rotation == 'cs':\n",
    "                    add_management_year(op_man, spin_up_corn, 2016)\n",
    "                    add_management_year(op_man, spin_up_soybean, 2017)\n",
    "                    add_management_year(op_man, spin_up_corn, 2018)\n",
    "                elif rotation == 'cc':\n",
    "                    add_management_year(op_man, spin_up_corn, 2018)\n",
    "                    add_management_year(op_man, spin_up_corn, 2016)\n",
    "                    add_management_year(op_man, spin_up_corn, 2017)\n",
    "                area.append( op_man.man_xml )\n",
    "                outfile = f'apsim_files/{county_name}_{field_id}_{soil_id}.apsim'\n",
    "                ### management data\n",
    "                tree = ElementTree()\n",
    "                tree._setroot( apsim_xml )\n",
    "                tree.write( outfile )\n",
    "            except (RuntimeError, TypeError, NameError):\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_apsim_files(greene, rots, dbconn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped.first()\n",
    "# field = grouped.get_group(25197).sort_values(by=['years'], ascending=True)\n",
    "# racc_field = raccoon.loc[raccoon['clukey'] == 2539919]\n",
    "#soil_row = soil_df.loc[soil_df['mukey'] == '410373']\n",
    "#soil_row['claytotal_r'][1]\n",
    "#soil_row.head()\n",
    "# racc_soils = racc_field.drop_duplicates('mukey')\n",
    "# for i in racc_soils['mukey']:\n",
    "#     soil = soil_df.loc[soil_df['mukey'] == i]\n",
    "#     #print(soil)\n",
    "# rotation_row = rots.loc[rots['clukey'] == 2539919]\n",
    "# rotation = rotation_row['rotation'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}